{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DACS Elevator Radio Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install all relied libraries\n",
    "#!pip install -r requirements.txt\n",
    "\n",
    "# Initialize\n",
    "from data.base import store_lyrics, read_cleaned_data\n",
    "from model.base import get_keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from billboard top 100 不要重新运行\n",
    "If this year's data is stored then it won't grab again. You can directly use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illegal input year. Setting year to be the current.\n",
      "Searching for \"Heat Waves\" by Glass Animals...\n",
      "Done.\n",
      "Searching for \"As It Was\" by Harry Styles...\n",
      "Done.\n",
      "Searching for \"Stay\" by The Kid LAROI & Justin Bieber...\n",
      "Done.\n",
      "Searching for \"Easy On Me\" by Adele...\n",
      "Done.\n",
      "Searching for \"Shivers\" by Ed Sheeran...\n",
      "Done.\n",
      "Searching for \"First Class\" by Jack Harlow...\n",
      "Done.\n",
      "Searching for \"Big Energy\" by Latto...\n",
      "Done.\n",
      "Searching for \"Ghost\" by Justin Bieber...\n",
      "Done.\n",
      "Searching for \"Super Gremlin\" by Kodak Black...\n",
      "Done.\n",
      "Searching for \"Cold Heart (PNAU Remix)\" by Elton John & Dua Lipa...\n",
      "Done.\n",
      "Searching for \"Wait For U\" by Future Featuring Drake & Tems...\n",
      "Done.\n",
      "Searching for \"About Damn Time\" by Lizzo...\n",
      "Done.\n",
      "Searching for \"Bad Habits\" by Ed Sheeran...\n",
      "Done.\n",
      "Searching for \"Thats What I Want\" by Lil Nas X...\n",
      "Done.\n",
      "Searching for \"Enemy\" by Imagine Dragons X JID...\n",
      "Done.\n",
      "Searching for \"Industry Baby\" by Lil Nas X & Jack Harlow...\n",
      "Done.\n",
      "Searching for \"abcdefu\" by GAYLE...\n",
      "Done.\n",
      "Searching for \"Need To Know\" by Doja Cat...\n",
      "Done.\n",
      "Searching for \"Wasted On You\" by Morgan Wallen...\n",
      "Done.\n",
      "Searching for \"Me Porto Bonito\" by Bad Bunny & Chencho Corleone...\n",
      "Done.\n",
      "Searching for \"Woman\" by Doja Cat...\n",
      "Done.\n",
      "Searching for \"Titi Me Pregunto\" by Bad Bunny...\n",
      "Done.\n",
      "Searching for \"Running Up That Hill (A Deal With God)\" by Kate Bush...\n",
      "Done.\n",
      "Searching for \"We Don't Talk About Bruno\" by Carolina Gaitan, Mauro Castillo, Adassa, Rhenzy Feliz, Diane Guerrero, Stephanie Beatriz & Encanto Cast...\n",
      "Done.\n",
      "Searching for \"Late Night Talking\" by Harry Styles...\n",
      "Done.\n",
      "Searching for \"I Like You (A Happier Song)\" by Post Malone Featuring Doja Cat...\n",
      "Done.\n",
      "Searching for \"You Proof\" by Morgan Wallen...\n",
      "Done.\n",
      "Searching for \"Bad Habit\" by Steve Lacy...\n",
      "Done.\n",
      "Searching for \"Sunroof\" by Nicky Youre & dazy...\n",
      "Done.\n",
      "Searching for \"One Right Now\" by Post Malone & The Weeknd...\n",
      "Done.\n",
      "Searching for \"Good 4 U\" by Olivia Rodrigo...\n",
      "Done.\n",
      "Searching for \"Numb Little Bug\" by Em Beihold...\n",
      "Done.\n",
      "Searching for \"Jimmy Cooks\" by Drake Featuring 21 Savage...\n",
      "Done.\n",
      "Searching for \"'Til You Can't\" by Cody Johnson...\n",
      "Done.\n",
      "Searching for \"Fancy Like\" by Walker Hayes...\n",
      "Done.\n",
      "Searching for \"The Kind Of Love We Make\" by Luke Combs...\n",
      "Done.\n",
      "Searching for \"I Ain't Worried\" by OneRepublic...\n",
      "Done.\n",
      "Searching for \"Break My Soul\" by Beyonce...\n",
      "Done.\n",
      "Searching for \"Something In The Orange\" by Zach Bryan...\n",
      "Done.\n",
      "Searching for \"Save Your Tears\" by The Weeknd & Ariana Grande...\n",
      "Done.\n",
      "Searching for \"Smokin Out The Window\" by Silk Sonic (Bruno Mars & Anderson .Paak)...\n",
      "Done.\n",
      "Searching for \"Levitating\" by Dua Lipa...\n",
      "Done.\n",
      "Searching for \"In A Minute\" by Lil Baby...\n",
      "Done.\n",
      "Searching for \"Moscow Mule\" by Bad Bunny...\n",
      "Done.\n",
      "Searching for \"You Right\" by Doja Cat & The Weeknd...\n",
      "Done.\n",
      "Searching for \"She Had Me At Heads Carolina\" by Cole Swindell...\n",
      "Done.\n",
      "Searching for \"Vegas\" by Doja Cat...\n",
      "Done.\n",
      "Searching for \"Pushin P\" by Gunna & Future Featuring Young Thug...\n",
      "Done.\n",
      "Searching for \"Buy Dirt\" by Jordan Davis Featuring Luke Bryan...\n",
      "Done.\n",
      "Searching for \"I Hate U\" by SZA...\n",
      "Done.\n",
      "Searching for \"Boyfriend\" by Dove Cameron...\n",
      "Done.\n",
      "Searching for \"Glimpse Of Us\" by Joji...\n",
      "Done.\n",
      "Searching for \"Surface Pressure\" by Jessica Darrow...\n",
      "Done.\n",
      "Searching for \"Fall In Love\" by Bailey Zimmerman...\n",
      "Done.\n",
      "Searching for \"Love Nwantiti (Ah Ah Ah)\" by CKay...\n",
      "Done.\n",
      "Searching for \"Super Freaky Girl\" by Nicki Minaj...\n",
      "Done.\n",
      "Searching for \"Hrs And Hrs\" by Muni Long...\n",
      "Done.\n",
      "Searching for \"Sand In My Boots\" by Morgan Wallen...\n",
      "Done.\n",
      "Searching for \"MAMIII\" by Becky G X Karol G...\n",
      "Done.\n",
      "Searching for \"Knife Talk\" by Drake Featuring 21 Savage & Project Pat...\n",
      "Done.\n",
      "Searching for \"AA\" by Walker Hayes...\n",
      "Done.\n",
      "Searching for \"Sweetest Pie\" by Megan Thee Stallion & Dua Lipa...\n",
      "Done.\n",
      "Searching for \"Provenza\" by Karol G...\n",
      "Done.\n",
      "Searching for \"Essence\" by Wizkid Featuring Justin Bieber & Tems...\n",
      "Done.\n",
      "Searching for \"All I Want For Christmas Is You\" by Mariah Carey...\n",
      "Done.\n",
      "Searching for \"Bam Bam\" by Camila Cabello Featuring Ed Sheeran...\n",
      "Done.\n",
      "Searching for \"5 Foot 9\" by Tyler Hubbard...\n",
      "Done.\n",
      "Searching for \"Get Into It (Yuh)\" by Doja Cat...\n",
      "Done.\n",
      "Searching for \"Efecto\" by Bad Bunny...\n",
      "Done.\n",
      "Searching for \"Rock And A Hard Place\" by Bailey Zimmerman...\n",
      "Done.\n",
      "Searching for \"Doin' This\" by Luke Combs...\n",
      "Done.\n",
      "Searching for \"Oh My God\" by Adele...\n",
      "Done.\n",
      "Searching for \"Better Days\" by NEIKED X Mae Muller X Polo G...\n",
      "Done.\n",
      "Searching for \"Meet Me At Our Spot\" by THE ANXIETY: WILLOW & Tyler Cole...\n",
      "Done.\n",
      "Searching for \"Fingers Crossed\" by Lauren Spencer-Smith...\n",
      "Done.\n",
      "Searching for \"All Too Well (Taylor's Version)\" by Taylor Swift...\n",
      "Done.\n",
      "Searching for \"Party\" by Bad Bunny & Rauw Alejandro...\n",
      "Done.\n",
      "Searching for \"Despues de La Playa\" by Bad Bunny...\n",
      "Done.\n",
      "Searching for \"You Should Probably Leave\" by Chris Stapleton...\n",
      "Done.\n",
      "Searching for \"Rockin' Around The Christmas Tree\" by Brenda Lee...\n",
      "Done.\n",
      "Searching for \"Broadway Girls\" by Lil Durk Featuring Morgan Wallen...\n",
      "Done.\n",
      "Searching for \"Take My Name\" by Parmalee...\n",
      "Done.\n",
      "Searching for \"What Happened To Virgil\" by Lil Durk Featuring Gunna...\n",
      "Done.\n",
      "Searching for \"Puffin On Zootiez\" by Future...\n",
      "Done.\n",
      "Searching for \"Like I Love Country Music\" by Kane Brown...\n",
      "Done.\n",
      "Searching for \"Jingle Bell Rock\" by Bobby Helms...\n",
      "Done.\n",
      "Searching for \"Ojitos Lindos\" by Bad Bunny & Bomba Estereo...\n",
      "Done.\n",
      "Searching for \"Trouble With A Heartbreak\" by Jason Aldean...\n",
      "Done.\n",
      "Searching for \"A Holly Jolly Christmas\" by Burl Ives...\n",
      "Done.\n",
      "Searching for \"Kiss Me More\" by Doja Cat Featuring SZA...\n",
      "Done.\n",
      "Searching for \"She Likes It\" by Russell Dickerson & Jake Scott...\n",
      "Done.\n",
      "Searching for \"Never Say Never\" by Cole Swindell / Lainey Wilson...\n",
      "Done.\n",
      "Searching for \"Damn Strait\" by Scotty McCreery...\n",
      "Done.\n",
      "Searching for \"She's All I Wanna Be\" by Tate McRae...\n",
      "Done.\n",
      "Searching for \"Last Night Lonely\" by Jon Pardi...\n",
      "Done.\n",
      "Searching for \"Flower Shops\" by ERNEST Featuring Morgan Wallen...\n",
      "Done.\n",
      "Searching for \"To The Moon!\" by JNR CHOI & Sam Tompkins...\n",
      "Done.\n",
      "Searching for \"Unholy\" by Sam Smith & Kim Petras...\n",
      "Done.\n",
      "Searching for \"One Mississippi\" by Kane Brown...\n",
      "Done.\n",
      "Searching for \"Circles Around This Town\" by Maren Morris...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "store_lyrics(year = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from a playlist from spotify 不要重新运行\n",
    "### How to find playlist ID:\n",
    "![playlist id](imgs/playlist_id.png)\n",
    "\n",
    "Or if you share the playlist with link, find the string before `?si=`:  \n",
    "https://open.spotify.com/playlist/**37i9dQZF1DX5Ejj0EkURtP**?si=a1e0243dd67c4cc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Apocalypse\" by Cigarettes After Sex...\n",
      "Done.\n",
      "Searching for \"Sunsetz\" by Cigarettes After Sex...\n",
      "Done.\n",
      "Searching for \"Cry\" by Cigarettes After Sex...\n",
      "Done.\n",
      "Searching for \"K.\" by Cigarettes After Sex...\n",
      "Done.\n",
      "Searching for \"My Jinji\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"I Know You Know I Love You\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"Jellyfish\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"我是一隻魚 I'm a Fish\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"Blues\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"Welcome To\" by Sunset Rollercoaster...\n",
      "Specified song does not contain lyrics. Rejecting.\n",
      "There's error when getting lyrcis of 'Welcome To' by Sunset Rollercoaster\n",
      "'NoneType' object has no attribute 'lyrics'\n",
      "Searching for \"Vanilla\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"Villa\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"I Wanna Be Yours\" by Arctic Monkeys...\n",
      "Done.\n",
      "Searching for \"I Wanna Be Adored - Remastered\" by The Stone Roses...\n",
      "No results found for: 'I Wanna Be Adored - Remastered The Stone Roses'\n",
      "There's error when getting lyrcis of 'I Wanna Be Adored - Remastered' by The Stone Roses\n",
      "'NoneType' object has no attribute 'lyrics'\n",
      "Searching for \"Sweet\" by Cigarettes After Sex...\n",
      "Done.\n",
      "Searching for \"Each Time You Fall in Love\" by Cigarettes After Sex...\n",
      "Done.\n",
      "Searching for \"You Might Be Sleeping\" by Jakob...\n",
      "Done.\n",
      "Searching for \"Candlelight\" by Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"有暖氣(You Nuan Chi)\" by Orange Ocean...\n",
      "Done.\n",
      "Searching for \"SLOW DANCING IN THE DARK\" by Joji...\n",
      "Done.\n",
      "Searching for \"海浪\" by deca joins...\n",
      "Done.\n",
      "Searching for \"Let There Be Light Again\" by 落日飛車 Sunset Rollercoaster...\n",
      "Done.\n",
      "Searching for \"Last Summer Whisper\" by Anri...\n",
      "Done.\n",
      "Searching for \"Perfect Sense\" by Arctic Monkeys...\n",
      "Done.\n",
      "Searching for \"J'aime pas le goût - A COLORS SHOW\" by LYNN...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "store_lyrics(playlist_id=\"1q2ztYCQCBrq14wFEJKOB8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '2019'\n",
    "lyrics_tokens = read_cleaned_data(address,stem_words = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'been', 'long', 'day', 'without', 'you', 'my', 'friend'], ['and', 'i', 'will', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you', 'again'], ['we', 'have', 'come', 'long', 'way', 'from', 'where', 'we', 'began'], ['oh', 'i', 'will', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you', 'again'], ['when', 'i', 'see', 'you', 'again'], ['dang', 'who', 'knew'], ['all', 'planes', 'we', 'flew'], ['good', 'things', 'we', 'have', 'been', 'through'], ['that', 'i', 'will', 'be', 'standing', 'right', 'here', 'talking', 'to', 'you'], ['bout', 'another', 'path'], ['i', 'know', 'we', 'loved', 'to', 'hit', 'road', 'and', 'laugh'], ['but', 'something', 'told', 'me', 'that', 'it', 'would', 'not', 'last'], ['had', 'to', 'switch', 'up'], ['look', 'at', 'things', 'different', 'see', 'bigger', 'picture'], ['those', 'were', 'days'], ['hard', 'work', 'forever', 'pays'], ['now', 'i', 'see', 'you', 'in', 'better', 'place', 'see', 'you', 'in', 'better', 'place'], ['uh'], ['how', 'can', 'we', 'not', 'talk', 'about', 'family', 'when', 'family', 'all', 'that', 'we', 'got'], ['everything', 'i', 'went', 'through', 'you', 'were', 'standing', 'there', 'by', 'my', 'side'], ['and', 'now', 'you', 'gon', 'be', 'with', 'me', 'for', 'last', 'ride'], ['it', 'been', 'long', 'day', 'without', 'you', 'my', 'friend'], ['and', 'i', 'will', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you', 'again'], ['we', 'have', 'come', 'long', 'way', 'from', 'where', 'we', 'began'], ['oh', 'i', 'will', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you', 'again'], ['when', 'i', 'see', 'you', 'again'], ['you', 'might', 'also', 'like', 'aah', 'oh', 'aah', 'oh'], ['wooooh-oh-oh-oh-oh-oh'], ['yeah'], ['first', 'you', 'both', 'go', 'out', 'your', 'way'], ['and', 'vibe', 'is', 'feeling', 'strong'], ['and', 'what', 'small', 'turn', 'to', 'friendship'], ['a', 'friendship', 'turn', 'to', 'bond'], ['and', 'that', 'bond', 'will', 'never', 'be', 'broken'], ['the', 'love', 'will', 'never', 'get', 'lost'], ['and', 'when', 'brotherhood', 'come', 'first'], ['then', 'line', 'will', 'never', 'be', 'crossed'], ['established', 'it', 'on', 'our', 'own'], ['when', 'that', 'line', 'had', 'to', 'be', 'drawn'], ['and', 'that', 'line', 'is', 'what', 'we', 'reach'], ['so', 'remember', 'me', 'when', 'i', 'am', 'gone'], ['how', 'can', 'we', 'not', 'talk', 'about', 'family', 'when', 'family', 'all', 'that', 'we', 'got'], ['everything', 'i', 'went', 'through', 'you', 'were', 'standing', 'there', 'by', 'my', 'side'], ['and', 'now', 'you', 'gon', 'be', 'with', 'me', 'for', 'last', 'ride'], ['so', 'let', 'light', 'guide', 'your', 'way', 'yeah'], ['hold', 'every', 'memory', 'as', 'you', 'go'], ['and', 'every', 'road', 'you', 'take', 'will', 'always', 'lead', 'you', 'home', 'home'], ['it', 'been', 'long', 'day', 'without', 'you', 'my', 'friend'], ['and', 'i', 'will', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you', 'again'], ['we', 'have', 'come', 'long', 'way', 'from', 'where', 'we', 'began'], ['oh', 'i', 'will', 'tell', 'you', 'all', 'about', 'it', 'when', 'i', 'see', 'you', 'again'], ['when', 'i', 'see', 'you', 'again'], ['aah', 'oh', 'aah', 'oh'], ['wooooh-oh-oh-oh-oh-oh'], ['yeah'], ['when', 'i', 'see', 'you', 'again'], ['see', 'you', 'again'], ['when', 'i', 'see', 'you', 'again'], ['aah', 'oh', 'aah', 'oh'], ['wooooh-oh-oh-oh-oh-oh'], ['yeah'], ['when', 'i', 'see', 'you', 'againembed']]\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_tokens[\"98_See You Again\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 100_Robbery has topic [[('gucci', 0.2425), ('slur', 0.2287), ('hennessy', 0.2276), ('dad', 0.2106), ('boy', 0.1999)]]\n",
      "Song 10_Sucker has topic [[('sucker', 0.2862), ('subliminal', 0.27), ('baby', 0.232), ('tattoo', 0.2251), ('road', 0.2133)]]\n",
      "Song 11_High Hopes has topic [[('hopes', 0.3455), ('destiny', 0.3086), ('prophecy', 0.3024), ('mama', 0.2691), ('rise', 0.2667)]]\n",
      "Song 12_Thank U Next has topic [[('song', 0.4449), ('songs', 0.3535), ('thanking', 0.3037), ('thankful', 0.2977), ('ricky', 0.2867)]]\n",
      "Song 13_Truth Hurts has topic [[('bitch', 0.3449), ('man', 0.2692), ('chick', 0.2595), ('boy', 0.2478), ('hair', 0.2321)]]\n",
      "Song 14_Dancing With A Stranger has topic [[('dancing', 0.2937), ('stranger', 0.2071), ('baby', 0.204), ('tonight', 0.1886), ('control', 0.1743)]]\n",
      "Song 15_Senorita has topic [[('señorita', 0.4001), ('runnin', 0.2693), ('like', 0.2257), ('kiss', 0.2127), ('tequila', 0.1906)]]\n",
      "Song 16_I Dont Care has topic [[('baby', 0.3189), ('party', 0.2981), ('dance', 0.2576), ('stay', 0.2125), ('nights', 0.2004)]]\n",
      "Song 17_Eastside has topic [[('backstreets', 0.2922), ('love', 0.2519), ('baby', 0.2509), ('kiss', 0.2235), ('singing', 0.2157)]]\n",
      "Song 18_Going Bad has topic [[('drizzy', 0.4032), ('lil', 0.3469), ('wheezy', 0.2842), ('bitch', 0.2666), ('dawg', 0.2649)]]\n",
      "Song 19_Shallow has topic [[('fallin', 0.3937), ('deep', 0.3687), ('dive', 0.3391), ('falling', 0.3344), ('shallow', 0.3192)]]\n",
      "Song 1_Old Town Road has topic [[('road', 0.3582), ('ride', 0.3292), ('riding', 0.3028), ('rodeo', 0.3023), ('town', 0.2926)]]\n",
      "Song 20_Better has topic [[('better', 0.3543), ('sober', 0.3062), ('drunk', 0.2353), ('high', 0.2227), ('night', 0.2221)]]\n",
      "Song 21_No Guidance has topic [[('raw', 0.3871), ('devvon', 0.3871), ('nigga', 0.3231), ('kd', 0.3146), ('roster', 0.2967)]]\n",
      "Song 22_Girls Like You has topic [[('backstreet', 0.3528), ('lovato', 0.332), ('timberlake', 0.3097), ('aguilera', 0.2895), ('christina', 0.2446)]]\n",
      "Song 23_Sweet But Psycho has topic [[('psycho', 0.4085), ('poison', 0.3729), ('sugarcoat', 0.3043), ('screamin', 0.2972), ('potions', 0.2426)]]\n",
      "Song 24_Suge has topic [[('gangster', 0.3483), ('nigga', 0.3404), ('bitch', 0.2883), ('lil', 0.2789), ('fool', 0.2774)]]\n",
      "Song 25_Middle Child has topic [[('rapper', 0.4993), ('rap', 0.4735), ('jigga', 0.4457), ('drizzy', 0.4363), ('lil', 0.4351)]]\n",
      "Song 26_Drip Too Hard has topic [[('nigga', 0.3763), ('runnin', 0.3695), ('lil', 0.3475), ('niggas', 0.2924), ('tryna', 0.2687)]]\n",
      "Song 27_Someone You Loved has topic [[('help', 0.3576), ('way', 0.267), ('numbed', 0.261), ('hear', 0.2585), ('need', 0.2416)]]\n",
      "Song 28_Ranom has topic [[('miner', 0.4514), ('minesom', 0.3442), ('diamond', 0.3361), ('noobies', 0.3327), ('mobs', 0.317)]]\n",
      "Song 29_If I Cant Have You has topic [[('song', 0.4239), ('texts', 0.2152), ('trying', 0.1626), ('forget', 0.156), ('distance', 0.1541)]]\n",
      "Song 2_Sunflower SpiderMan Into The SpiderVerse has topic [[('wreck', 0.3151), ('tryna', 0.2256), ('baby', 0.2222), ('trip', 0.2062), ('crash', 0.2061)]]\n",
      "Song 30_Goodbyes has topic [[('kurt', 0.3272), ('nicki', 0.262), ('cut', 0.2507), ('goodbyes', 0.2442), ('edge', 0.2424)]]\n",
      "Song 31_ZEZE has topic [[('nigga', 0.3415), ('niggas', 0.2723), ('mansion', 0.2632), ('streets', 0.2575), ('zombie', 0.2414)]]\n",
      "Song 32_Better Now has topic [[('better', 0.2367), ('woulda', 0.2175), ('jonas', 0.2063), ('tryna', 0.2047), ('knew', 0.19)]]\n",
      "Song 33_Youngblood has topic [[('youngblood', 0.3875), ('baby', 0.3012), ('love', 0.2457), ('beat', 0.2426), ('goodbye', 0.2339)]]\n",
      "Song 34_Money In The Grave has topic [[('lil', 0.3964), ('nigga', 0.3483), ('tryna', 0.297), ('cc', 0.2925), ('niggas', 0.285)]]\n",
      "Song 35_Speechless has topic [[('baby', 0.2439), ('room', 0.2369), ('daze', 0.2226), ('perfume', 0.2182), ('nervous', 0.2072)]]\n",
      "Song 36_Break Up With Your Girlfriend Im Bored has topic [[('girlfriend', 0.3628), ('songs', 0.2354), ('just', 0.2125), ('break', 0.2052), ('singing', 0.1963)]]\n",
      "Song 37_Please Me has topic [[('baby', 0.3528), ('twerk', 0.3274), ('tease', 0.3209), ('beggin', 0.2872), ('twerking', 0.2858)]]\n",
      "Song 38_Money has topic [[('nigga', 0.3183), ('lil', 0.3079), ('flow', 0.285), ('da', 0.2833), ('bitch', 0.2779)]]\n",
      "Song 39_You Need To Calm Down has topic [[('calm', 0.3828), ('patrón', 0.3236), ('stop', 0.3097), ('somebody', 0.2863), ('shade', 0.2848)]]\n",
      "Song 3_Without Me has topic [[('lonely', 0.215), ('love', 0.1951), ('baby', 0.171), ('high', 0.1688), ('feel', 0.1639)]]\n",
      "Song 40_Panini has topic [[('meanie', 0.2914), ('meaning', 0.2748), ('mean', 0.2684), ('demands', 0.2578), ('panini', 0.2471)]]\n",
      "Song 41_Look Back At It has topic [[('da', 0.2843), ('nigga', 0.2781), ('melody', 0.2637), ('girls', 0.2571), ('throwin', 0.2557)]]\n",
      "Song 42_A Lot has topic [[('rap', 0.421), ('album', 0.3567), ('rappers', 0.31), ('nigga', 0.3084), ('rapping', 0.2946)]]\n",
      "Song 43_ME has topic [[('promise', 0.2786), ('baby', 0.2545), ('doll', 0.2454), ('band', 0.237), ('girl', 0.2285)]]\n",
      "Song 44_MIA has topic [[('romeo', 0.3775), ('baby', 0.2973), ('sing', 0.2684), ('bunny', 0.2391), ('spiral', 0.2374)]]\n",
      "Song 45_Pop Out has topic [[('gang', 0.4121), ('killers', 0.3664), ('lil', 0.3396), ('nigga', 0.3349), ('killer', 0.3238)]]\n",
      "Song 46_Beautiful Crazy has topic [[('wild', 0.3092), ('crazy', 0.2753), ('unforgettable', 0.2645), ('dances', 0.2341), ('unpredictable', 0.2237)]]\n",
      "Song 47_Thotiana has topic [[('nigga', 0.3333), ('runnin', 0.3187), ('gang', 0.3184), ('thotiana', 0.3063), ('smackin', 0.2918)]]\n",
      "Song 48_Lucid Dreams has topic [[('heartbreak', 0.2835), ('grave', 0.2254), ('listened', 0.2086), ('okay', 0.2062), ('enviyon', 0.1971)]]\n",
      "Song 49_Mo Bamba has topic [[('drake', 0.441), ('nigga', 0.4244), ('niggas', 0.3326), ('bitch', 0.3199), ('drug', 0.2876)]]\n",
      "Song 4_Bad Guy has topic [[('lyrics', 0.433), ('mama', 0.4195), ('mommy', 0.3653), ('song', 0.3576), ('girlfriend', 0.2941)]]\n",
      "Song 50_Beautiful People has topic [[('fashion', 0.309), ('conversation', 0.3025), ('surrounded', 0.2961), ('party', 0.2632), ('dear', 0.2476)]]\n",
      "Song 51_Wake Up In The Sky has topic [[('fly', 0.4508), ('gucci', 0.4293), ('lil', 0.3599), ('flying', 0.3438), ('yo', 0.3284)]]\n",
      "Song 52_Whiskey Glasses has topic [[('whiskey', 0.3254), ('karaoke', 0.2997), ('heartbreak', 0.2788), ('drunk', 0.2689), ('drinks', 0.2677)]]\n",
      "Song 53_Gods Country has topic [[('dixie', 0.3902), ('country', 0.3545), ('georgia', 0.3516), ('church', 0.331), ('town', 0.2933)]]\n",
      "Song 54_Be Alright has topic [[('teary', 0.2727), ('messages', 0.2521), ('betrayal', 0.2171), ('okay', 0.2096), ('kissed', 0.2078)]]\n",
      "Song 55_Pure Water has topic [[('rap', 0.4822), ('nigga', 0.3751), ('lil', 0.3532), ('runnin', 0.3326), ('gang', 0.321)]]\n",
      "Song 56_The Git Up has topic [[('boogie', 0.3769), ('slide', 0.3725), ('roll', 0.3532), ('hips', 0.3209), ('hoedown', 0.3166)]]\n",
      "Song 57_Taki Taki has topic [[('pantiesito', 0.3531), ('taki', 0.3428), ('punani', 0.3416), ('pasito', 0.3123), ('trabaje', 0.309)]]\n",
      "Song 58_CloseToMe has topic [[('trouble', 0.2573), ('animal', 0.2495), ('undercover', 0.239), ('bitch', 0.2305), ('jungle', 0.2136)]]\n",
      "Song 59_Envy Me has topic [[('nigga', 0.3892), ('lil', 0.3527), ('gang', 0.3515), ('trappin', 0.3295), ('niggas', 0.3188)]]\n",
      "Song 5_Wow has topic [[('lil', 0.3979), ('dre', 0.3278), ('grinnin', 0.3061), ('flockin', 0.2893), ('pocket', 0.2839)]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_keywords(lyrics_tokens)\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Desktop\\DACS-Elevator-Radio-Producer\\model\\base.py:8\u001b[0m, in \u001b[0;36mget_keywords\u001b[1;34m(lyrics, model, n_gram, word_no)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m lyrics\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m      7\u001b[0m     this_keywords \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m     \u001b[39mif\u001b[39;00m(model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert\u001b[39m\u001b[39m'\u001b[39m): this_keywords\u001b[39m.\u001b[39mappend(bert_based\u001b[39m.\u001b[39;49mget_keybert(lyrics_tokens\u001b[39m=\u001b[39;49mlyrics[key], n_gram\u001b[39m=\u001b[39;49mn_gram,word_no\u001b[39m=\u001b[39;49mword_no))\n\u001b[0;32m      9\u001b[0m     \u001b[39melif\u001b[39;00m(model \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m): this_keywords\u001b[39m.\u001b[39mappend(tfidf\u001b[39m.\u001b[39mget_n_words(lyrics_tokens\u001b[39m=\u001b[39mlyrics[key], n_gram\u001b[39m=\u001b[39mn_gram,word_no\u001b[39m=\u001b[39mword_no))\n\u001b[0;32m     10\u001b[0m     keywords[key] \u001b[39m=\u001b[39m this_keywords\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Desktop\\DACS-Elevator-Radio-Producer\\model\\bert_based.py:5\u001b[0m, in \u001b[0;36mget_keybert\u001b[1;34m(lyrics_tokens, n_gram, word_no)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_keybert\u001b[39m(lyrics_tokens:\u001b[39mlist\u001b[39m[\u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]], n_gram \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), word_no \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m):\n\u001b[0;32m      4\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m lyrics_tokens])\n\u001b[1;32m----> 5\u001b[0m     kw_model \u001b[39m=\u001b[39m KeyBERT(\u001b[39m'\u001b[39;49m\u001b[39mall-mpnet-base-v2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m     keywords \u001b[39m=\u001b[39m kw_model\u001b[39m.\u001b[39mextract_keywords(doc, top_n\u001b[39m=\u001b[39mword_no, keyphrase_ngram_range\u001b[39m=\u001b[39mn_gram)\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m keywords\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\keybert\\_model.py:55\u001b[0m, in \u001b[0;36mKeyBERT.__init__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"KeyBERT initialization\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m                  * https://www.sbert.net/docs/pretrained_models.html\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m select_backend(model)\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\keybert\\backend\\_utils.py:49\u001b[0m, in \u001b[0;36mselect_backend\u001b[1;34m(embedding_model)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m# Create a Sentence Transformer model based on a string\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding_model, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m SentenceTransformerBackend(embedding_model)\n\u001b[0;32m     51\u001b[0m \u001b[39m# Hugging Face embeddings\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding_model, Pipeline):\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py:42\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.__init__\u001b[1;34m(self, embedding_model)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m embedding_model\n\u001b[0;32m     41\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding_model, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m SentenceTransformer(embedding_model)\n\u001b[0;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a correct SentenceTransformers model: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`from sentence_transformers import SentenceTransformer` \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`model = SentenceTransformer(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[0;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[0;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[0;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39mfor\u001b[39;00m module_config \u001b[39min\u001b[39;00m modules_config:\n\u001b[0;32m    839\u001b[0m     module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 840\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m    841\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[0;32m    843\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[0;32m    136\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[1;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m Transformer(model_name_or_path\u001b[39m=\u001b[39minput_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[0;32m     28\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[1;32m---> 29\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model(model_name_or_path, config, cache_dir)\n\u001b[0;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;00m tokenizer_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39mcache_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_args)\n\u001b[0;32m     33\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, config\u001b[39m=\u001b[39;49mconfig, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:467\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    466\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 467\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    468\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    469\u001b[0m     )\n\u001b[0;32m    470\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    471\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\transformers\\modeling_utils.py:2611\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2608\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2610\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2611\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2613\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[0;32m   2614\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:488\u001b[0m, in \u001b[0;36mMPNetModel.__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m    486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m--> 488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m MPNetEmbeddings(config)\n\u001b[0;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m MPNetEncoder(config)\n\u001b[0;32m    490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39m=\u001b[39m MPNetPooler(config) \u001b[39mif\u001b[39;00m add_pooling_layer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:79\u001b[0m, in \u001b[0;36mMPNetEmbeddings.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(config\u001b[39m.\u001b[39;49mvocab_size, config\u001b[39m.\u001b[39;49mhidden_size, padding_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx)\n\u001b[0;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(\n\u001b[0;32m     81\u001b[0m     config\u001b[39m.\u001b[39mmax_position_embeddings, config\u001b[39m.\u001b[39mhidden_size, padding_idx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_eps)\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs),\n\u001b[0;32m    143\u001b[0m                             requires_grad\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _freeze)\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n\u001b[0;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlist\u001b[39m(_weight\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[0;32m    147\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:153\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     init\u001b[39m.\u001b[39;49mnormal_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\torch\\nn\\init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[1;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[1;32mc:\\Users\\P70086556\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\torch\\nn\\init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_keywords(lyrics_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
